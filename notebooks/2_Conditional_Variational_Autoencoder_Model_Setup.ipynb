{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOh4zNVt3kBobylFyFOugLb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Tutorial 2: Setting up the Conditional Variational Autoencoder in Pytorch"],"metadata":{"id":"VZyMYMNowjfm"}},{"cell_type":"markdown","source":["## Conditional Variational Autoencoder Architecture"],"metadata":{"id":"DmiYJruIzcod"}},{"cell_type":"markdown","source":["## Defining the Encoder"],"metadata":{"id":"Hu9zmbtGzhDI"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, latent_dim=32, label_dim=1):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(1, 32, 4, 2, 1), nn.ReLU(),\n","            nn.Conv2d(32, 64, 4, 2, 1), nn.ReLU(),\n","            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(),\n","        )\n","        self.flatten = nn.Flatten()\n","\n","        self.fc_mu = nn.Linear(128*16*16 + label_dim, latent_dim)\n","        self.fc_logvar = nn.Linear(128*16*16 + label_dim, latent_dim)\n","\n","    def forward(self, x, y):\n","        h = self.conv(x)\n","        h = self.flatten(h)\n","        h = torch.cat([h, y], dim=1)\n","        return self.fc_mu(h), self.fc_logvar(h)\n"],"metadata":{"id":"CX3zn04SzlTW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Defining the Decoder"],"metadata":{"id":"3pKV2gcNzmWx"}},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, latent_dim=32, label_dim=1):\n","        super().__init__()\n","        self.fc = nn.Linear(latent_dim + label_dim, 128*16*16)\n","        self.deconv = nn.Sequential(\n","            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),\n","            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.ReLU(),\n","            nn.ConvTranspose2d(32, 1, 4, 2, 1), nn.Sigmoid(),\n","        )\n","\n","    def forward(self, z, y):\n","        h = torch.cat([z, y], dim=1)\n","        h = self.fc(h).view(-1, 128, 16, 16)\n","        return self.deconv(h)\n"],"metadata":{"id":"jDCzRz9nwpSX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Defining the CVAE class"],"metadata":{"id":"qBj5lvULzsaw"}},{"cell_type":"code","source":["class CVAE(nn.Module):\n","    def __init__(self, latent_dim=32, label_dim=1):\n","        super().__init__()\n","        self.encoder = Encoder(latent_dim, label_dim)\n","        self.decoder = Decoder(latent_dim, label_dim)\n","\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","\n","    def forward(self, x, y):\n","        mu, logvar = self.encoder(x, y)\n","        z = self.reparameterize(mu, logvar)\n","        recon = self.decoder(z, y)\n","        return recon, mu, logvar\n","\n","\n","def cvae_loss(recon_x, x, mu, logvar):\n","    batch_size = x.size(0)\n","\n","    recon_loss = nn.functional.mse_loss(\n","        recon_x, x, reduction='sum'\n","    ) / batch_size\n","\n","    kl = -0.5 * torch.sum(\n","        1 + logvar - mu.pow(2) - logvar.exp()\n","    ) / batch_size\n","\n","    return recon_loss + kl\n","\n","\n","def evaluate(model, loader, device):\n","    model.eval()\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","        for imgs, labels in loader:\n","            imgs = imgs.to(device)\n","            labels = labels.to(device)\n","\n","            recon, mu, logvar = model(imgs, labels)\n","            loss = cvae_loss(recon, imgs, mu, logvar)\n","            total_loss += loss.item()\n","\n","    return total_loss / len(loader)"],"metadata":{"id":"B1SB5-rLzbUk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"qsc_rAK40C0K"}}]}