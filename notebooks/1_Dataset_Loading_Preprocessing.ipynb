{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBjxS5HVVu5w"
   },
   "source": [
    "# Tutorial 1: Loading the CheXpert Dataset and Data Preprocessing\n",
    "\n",
    "This tutorial demonstrates how to download, subset, preprocess, and load the CheXpert chest X-ray dataset for use in PyTorch models.  \n",
    "\n",
    "Because the full CheXpert dataset is very large, we focus on:\n",
    "- Using the **CheXpert-v1.0-small** subset\n",
    "- Copying a **manageable subset of images** into Google Colab\n",
    "- Filtering image metadata to ensure consistency between images and labels\n",
    "- Preparing the dataset for training\n",
    "\n",
    "By the end of this tutorial, we will have a clean PyTorch `Dataset` class that can be used directly for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "CheXpert is a large-scale chest X-ray dataset containing over 200,000 images annotated for 14 clinical observations.  \n",
    "Each image is associated with metadata stored in CSV files, including:\n",
    "- Patient and study identifiers\n",
    "- View position (frontal or lateral)\n",
    "- Labels indicating the presence, absence, or uncertainty of various pathologies\n",
    "\n",
    "The original dataset is available for access at https://stanfordaimi.azurewebsites.net/datasets/8cbd9ed4-2eb9-4565-affc-111cf4f7ebe2, but is extremely large, and for the purposes of this tutorial, we access the smaller, subset of the data available on Kaggle.\n",
    "\n",
    "In this tutorial we focus on:\n",
    "- **Frontal chest X-rays only**\n",
    "- A **single binary label (Pneumonia)** for simplicity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7F2V9MaavI9"
   },
   "source": [
    "## Downloading the Dataset\n",
    "\n",
    "We download the CheXpert small dataset from Kaggle using the Kaggle API.  \n",
    "Google Colab is used to avoid local installation issues and to enable GPU acceleration later.\n",
    "\n",
    "This step requires:\n",
    "- A Kaggle account\n",
    "- A `kaggle.json` API token placed in Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38513,
     "status": "ok",
     "timestamp": 1765405208759,
     "user": {
      "displayName": "Bethany Pena",
      "userId": "12783154964209810181"
     },
     "user_tz": 420
    },
    "id": "6k1kuk_JVTym",
    "outputId": "8f0d5629-e1c2-47fc-a4fd-e9fbca02b909"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# change in configs/paths.py to your desired dir in Google Drive\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# change in configs/paths.py to your desired dir in Google Drive\n",
    "from configs.paths import GOOGLE_DRIVE_ROOT\n",
    "\n",
    "# download dataset\n",
    "if os.path.exists(os.path.join(GOOGLE_DRIVE_ROOT, \"CheXpert-v1.0-small\")):\n",
    "    print(\"CheXpert already downloaded: skipping.\")\n",
    "    return\n",
    "else:\n",
    "    !kaggle datasets download -d ashery/chexpert\n",
    "    !unzip chexpert.zip -d chexpert\n",
    "    print(\"CheXpert not found: downloading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGeq3RfRa7oY"
   },
   "source": [
    "## Upload a Subset to Colab\n",
    "\n",
    "We want to upload the dataset to Colab memory to prevent slowdowns from I/O to Google Drive during the training process. The CheXpert dataset is too large to fully load into Colab memory.  \n",
    "\n",
    "To enable fast experimentation, we copy only a **subset of patient training data folders** into the Colab filesystem. Feel free to experiment with the subset size you upload into Colab.\n",
    "\n",
    "Key steps:\n",
    "- Copy data in batches to avoid I/O errors\n",
    "- Skip files that already exist\n",
    "- Retry failed transfers\n",
    "- Keep the original directory structure intact\n",
    "\n",
    "This allows PyTorch to load images efficiently during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgrcJHQ7lTi3"
   },
   "outputs": [],
   "source": [
    "# modify paths in configs/paths.py to yours if needed\n",
    "from config.paths import (GOOGLE_DRIVE_CHEXPERT_VALID_DIR, \n",
    "                            GOOGLE_DRIVE_CHEXPERT_TRAIN_CSV, \n",
    "                            GOOGLE_DRIVE_CHEXPERT_VALID_CSV,\n",
    "                            CHEXPERT_VALID_DIR,\n",
    "                            CHEXPERT_TRAIN_CSV,\n",
    "                            CHEXPERT_VALID_DIR)\n",
    "\n",
    "!rsync -avh --ignore-errors {GOOGLE_DRIVE_CHEXPERT_VALID_DIR} {CHEXPERT_VALID_DIR}\n",
    "!rsync -avh --ignore-errors {GOOGLE_DRIVE_CHEXPERT_TRAIN_CSV} {CHEXPERT_TRAIN_CSV}\n",
    "!rsync -avh --ignore-errors {GOOGLE_DRIVE_CHEXPERT_VALID_CSV} {CHEXPERT_VALID_CSV}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V09ENbGXlomO"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Modify if needed \n",
    "BATCH_SIZE = 50\n",
    "TOTAL = 2500 \n",
    "\n",
    "def batch_copy_patients(src, dst, batch_size, total):\n",
    "    os.makedirs(dst, exist_ok=True)\n",
    "\n",
    "    patients = sorted(os.listdir(src))[:total]\n",
    "    print(f\"Found {len(patients)} patient folders. Starting batch copy...\")\n",
    "\n",
    "    for i in range(0, len(patients), batch_size):\n",
    "        batch = patients[i:i + batch_size]\n",
    "        print(f\"\\n=== Copying batch {i//batch_size + 1} ({i} to {i+len(batch)-1}) ===\")\n",
    "\n",
    "        for p in batch:\n",
    "            src_path = os.path.join(src, p)\n",
    "            dst_path = os.path.join(dst, p)\n",
    "\n",
    "            if os.path.exists(dst_path):\n",
    "                print(f\"{p} already exists â€” skipping\")\n",
    "                continue\n",
    "\n",
    "            retries = 3\n",
    "            for attempt in range(1, retries + 1):\n",
    "                print(f\"Copying {p} (attempt {attempt})...\")\n",
    "\n",
    "                result = subprocess.run(\n",
    "                    [\"rsync\", \"-a\", src_path, dst],\n",
    "                    stdout=subprocess.PIPE,\n",
    "                    stderr=subprocess.PIPE,\n",
    "                    text=True\n",
    "                )\n",
    "\n",
    "                if result.returncode == 0:\n",
    "                    print(f\"Finished {p}\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Error copying {p}: {result.stderr.strip()}\")\n",
    "                    time.sleep(5)\n",
    "\n",
    "            if result.returncode != 0:\n",
    "                print(f\"Failed to copy {p} after {retries} attempts.\")\n",
    "\n",
    "\n",
    "batch_copy_patients(\n",
    "        GOOGLE_DRIVE_CHEXPERT_TRAIN_DIR,\n",
    "        CHEXPERT_TRAIN_DIR, \n",
    "        BATCH_SIZE,\n",
    "        TOTAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tg-yOHV7lyIZ"
   },
   "source": [
    "## Synchronizing Image Files and Metadata (.csv files)\n",
    "\n",
    "CheXpert stores labels in CSV files that reference image paths.  \n",
    "After copying only a subset of images, many CSV entries may point to files that no longer exist. We need the CSV to be consistent with the training data directory structure so that we don't run into errors in our Dataset class.\n",
    "\n",
    "Additionally, we will train on frontal views only, since this is a singular distribution.\n",
    "\n",
    "To ensure consistency:\n",
    "- We scan the copied image directory to collect valid image paths\n",
    "- We filter the CSV to keep only rows corresponding to existing images\n",
    "- We keep **frontal views only** \n",
    "- We generate a new `train_subset.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVb2DpV0lwQH"
   },
   "outputs": [],
   "source": [
    "def collect_valid_paths(root):\n",
    "    valid = set()\n",
    "    for root_dir, _, files in os.walk(root):\n",
    "        for f in files:\n",
    "            if f.endswith(\".jpg\"):\n",
    "                rel = os.path.relpath(os.path.join(root_dir, f), root)\n",
    "                valid.add(rel)\n",
    "    return valid\n",
    "    \n",
    "valid_paths = collect_valid_paths(CHEXPERT_TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1107,
     "status": "ok",
     "timestamp": 1765613774131,
     "user": {
      "displayName": "Bethany Pena",
      "userId": "12783154964209810181"
     },
     "user_tz": 420
    },
    "id": "N2iF-TJResjo",
    "outputId": "cd3af344-3b29-4cbf-a449-cdc6efc67a2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset size: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# modify in configs/paths.py if needed\n",
    "from configs.paths import DEFAULT_DATA_ROOT\n",
    "\n",
    "def prepare_train_csv(\n",
    "    image_root = DEFAULT_DATA_ROOT,\n",
    "    csv_in     = CHEXPERT_TRAIN_CSV,\n",
    "    csv_out    = CHEXPERT_TRAIN_SUBSET_CSV\n",
    "):\n",
    "    print(\"Collecting valid image paths...\")\n",
    "    valid_paths = collect_valid_paths(image_root)\n",
    "\n",
    "    print(\"Loading CSV...\")\n",
    "    df = pd.read_csv(csv_in).fillna(0)\n",
    "\n",
    "    # Normalize paths\n",
    "    df[\"Path\"] = df[\"Path\"].str.replace(\n",
    "        \"CheXpert-v1.0-small/train/\", \"\", regex=False\n",
    "    )\n",
    "\n",
    "    # Keep frontal views only\n",
    "    df = df[df[\"Path\"].str.contains(\"frontal\", na=False)]\n",
    "\n",
    "    # Keep only images that exist on colab\n",
    "    df = df[df[\"Path\"].isin(valid_paths)]\n",
    "\n",
    "    df.to_csv(csv_out, index=False)\n",
    "\n",
    "    print(f\"Filtered dataset size: {len(df)}\")\n",
    "    print(\"Example CSV path:\", df[\"Path\"].iloc[0])\n",
    "    print(\"Example valid path:\", next(iter(valid_paths)))\n",
    "\n",
    "prepare_train_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBUZ-yn1wzAW"
   },
   "source": [
    "## PyTorch Dataset Class\n",
    "\n",
    "We define a custom PyTorch `Dataset` class to:\n",
    "- Load data\n",
    "- Apply basic preprocessing transforms\n",
    "- Return `(image, label)` pairs suitable for training\n",
    "\n",
    "Design choices:\n",
    "- Grayscale images (single channel)\n",
    "- Simple resizing and tensor conversion\n",
    "- Handle by missing files by warning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1765647019360,
     "user": {
      "displayName": "Bethany Pena",
      "userId": "12783154964209810181"
     },
     "user_tz": 420
    },
    "id": "m5siUnSnw1me"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "\n",
    "class CheXpertDataset(Dataset):\n",
    "    def __init__(self, csv_path, root, label_name=\"Pneumonia\"):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.df = self.df.fillna(0)\n",
    "\n",
    "        # Keep only rows with valid image paths\n",
    "        self.df = self.df[self.df['Path'].notna()]\n",
    "\n",
    "        self.root = root\n",
    "        self.label_name = label_name\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        rel_path = row[\"Path\"].replace(\"CheXpert-v1.0-small/\", \"\")\n",
    "        img_path = os.path.join(self.root, rel_path)\n",
    "\n",
    "        # Skip missing images\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Missing image: {img_path}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        img = self.transform(img)\n",
    "\n",
    "        # Labels are -1,0,1 in CheXpert -> convert to {0,1}\n",
    "        y = torch.tensor([1.0 if row[self.label_name] == 1 else 0.0], dtype=torch.float32)\n",
    "\n",
    "        return img, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "At this stage, we have:\n",
    "- Downloaded and subsetted the CheXpert dataset\n",
    "- Ensured consistency between image files and CSV metadata\n",
    "- Filtered to frontal views only\n",
    "- Created a PyTorch-compatible dataset class\n",
    "\n",
    "This dataset will be used in subsequent tutorials for:\n",
    "- Model definition\n",
    "- Training conditional variational autoencoders\n",
    "- Evaluating reconstruction quality\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNULCaGvURQR4Gp1fUUOyIF",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
